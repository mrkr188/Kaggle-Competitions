{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "import nltk\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, \\\n",
    "                        Flatten, MaxPooling1D, GRU, SpatialDropout1D, Bidirectional, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.&lt;br /&gt;&lt;br /&gt;Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.&lt;br /&gt;&lt;br /&gt;The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.&lt;br /&gt;&lt;br /&gt;Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.&lt;br /&gt;&lt;br /&gt;Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.&lt;br /&gt;&lt;br /&gt;The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this film (\\the greatest filmed opera ever,\\\" didn't I read somewhere?) either don't care for opera, don't care for Wagner, or don't care about anything except their desire to appear Cultured. Either as a representation of Wagner's swan-song, or as a movie, this strikes me as an unmitigated disaster, with a leaden reading of the score matched to a tricksy, lugubrious realisation of the text.&lt;br /&gt;&lt;br /&gt;It's questionable that people with ideas as to what an opera (or, for that matter, a play, especially one by Shakespeare) is \\\"about\\\" should be allowed anywhere near a theatre or film studio; Syberberg, very fashionably, but without the smallest justification from Wagner's text, decided that Parsifal is \\\"about\\\" bisexual integration, so that the title character, in the latter stages, transmutes into a kind of beatnik babe, though one who continues to sing high tenor -- few if any of the actors in the film are the singers, and we get a double dose of Armin Jordan, the conductor, who is seen as the face (but not heard as the voice) of Amfortas, and also appears monstrously in double exposure as a kind of Batonzilla or Conductor Who Ate Monsalvat during the playing of the Good Friday music -- in which, by the way, the transcendant loveliness of nature is represented by a scattering of shopworn and flaccid crocuses stuck in ill-laid turf, an expedient which baffles me. In the theatre we sometimes have to piece out such imperfections with our thoughts, but I can't think why Syberberg couldn't splice in, for Parsifal and Gurnemanz, mountain pasture as lush as was provided for Julie Andrews in Sound of Music...&lt;br /&gt;&lt;br /&gt;The sound is hard to endure, the high voices and the trumpets in particular possessing an aural glare that adds another sort of fatigue to our impatience with the uninspired conducting and paralytic unfolding of the ritual. Someone in another review mentioned the 1951 Bayreuth recording, and Knappertsbusch, though his tempi are often very slow, had what Jordan altogether lacks, a sense of pulse, a feeling for the ebb and flow of the music -- and, after half a century, the orchestral sound in that set, in modern pressings, is still superior to this film.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 80's exploitation, hooray! The pre-credits opening sequences somewhat give the false impression that we're dealing with a serious and harrowing drama, but you need not fear because barely ten minutes later we're up until our necks in nonsensical chainsaw battles, rough fist-fights, lurid dialogs and gratuitous nudity! Bo and Ingrid are two orphaned siblings with an unusually close and even slightly perverted relationship. Can you imagine playfully ripping off the towel that covers your sister's naked body and then stare at her unshaven genitals for several whole minutes? Well, Bo does that to his sister and, judging by her dubbed laughter, she doesn't mind at all. Sick, dude! Anyway, as kids they fled from Russia with their parents, but nasty soldiers brutally slaughtered mommy and daddy. A friendly smuggler took custody over them, however, and even raised and trained Bo and Ingrid into expert smugglers. When the actual plot lifts off, 20 years later, they're facing their ultimate quest as the mythical and incredibly valuable White Fire diamond is coincidentally found in a mine. Very few things in life ever made as little sense as the plot and narrative structure of \\White Fire\\\", but it sure is a lot of fun to watch. Most of the time you have no clue who's beating up who or for what cause (and I bet the actors understood even less) but whatever! The violence is magnificently grotesque and every single plot twist is pleasingly retarded. The script goes totally bonkers beyond repair when suddenly  and I won't reveal for what reason  Bo needs a replacement for Ingrid and Fred Williamson enters the scene with a big cigar in his mouth and his sleazy black fingers all over the local prostitutes. Bo's principal opponent is an Italian chick with big breasts but a hideous accent, the preposterous but catchy theme song plays at least a dozen times throughout the film, there's the obligatory \\\"we're-falling-in-love\\\" montage and loads of other attractions! My God, what a brilliant experience. The original French title translates itself as \\\"Life to Survive\\\", which is uniquely appropriate because it makes just as much sense as the rest of the movie: None!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment  \\\n",
       "0  5814_8  1           \n",
       "1  2381_9  1           \n",
       "2  7759_3  0           \n",
       "3  3630_4  0           \n",
       "4  9495_8  1           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              review  \n",
       "0  With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.                                                                                                                                                     \n",
       "1  \\The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2  The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.  \n",
       "3  It must be assumed that those who praised this film (\\the greatest filmed opera ever,\\\" didn't I read somewhere?) either don't care for opera, don't care for Wagner, or don't care about anything except their desire to appear Cultured. Either as a representation of Wagner's swan-song, or as a movie, this strikes me as an unmitigated disaster, with a leaden reading of the score matched to a tricksy, lugubrious realisation of the text.<br /><br />It's questionable that people with ideas as to what an opera (or, for that matter, a play, especially one by Shakespeare) is \\\"about\\\" should be allowed anywhere near a theatre or film studio; Syberberg, very fashionably, but without the smallest justification from Wagner's text, decided that Parsifal is \\\"about\\\" bisexual integration, so that the title character, in the latter stages, transmutes into a kind of beatnik babe, though one who continues to sing high tenor -- few if any of the actors in the film are the singers, and we get a double dose of Armin Jordan, the conductor, who is seen as the face (but not heard as the voice) of Amfortas, and also appears monstrously in double exposure as a kind of Batonzilla or Conductor Who Ate Monsalvat during the playing of the Good Friday music -- in which, by the way, the transcendant loveliness of nature is represented by a scattering of shopworn and flaccid crocuses stuck in ill-laid turf, an expedient which baffles me. In the theatre we sometimes have to piece out such imperfections with our thoughts, but I can't think why Syberberg couldn't splice in, for Parsifal and Gurnemanz, mountain pasture as lush as was provided for Julie Andrews in Sound of Music...<br /><br />The sound is hard to endure, the high voices and the trumpets in particular possessing an aural glare that adds another sort of fatigue to our impatience with the uninspired conducting and paralytic unfolding of the ritual. Someone in another review mentioned the 1951 Bayreuth recording, and Knappertsbusch, though his tempi are often very slow, had what Jordan altogether lacks, a sense of pulse, a feeling for the ebb and flow of the music -- and, after half a century, the orchestral sound in that set, in modern pressings, is still superior to this film.\"                                                                                                                                                                                                              \n",
       "4  Superbly trashy and wondrously unpretentious 80's exploitation, hooray! The pre-credits opening sequences somewhat give the false impression that we're dealing with a serious and harrowing drama, but you need not fear because barely ten minutes later we're up until our necks in nonsensical chainsaw battles, rough fist-fights, lurid dialogs and gratuitous nudity! Bo and Ingrid are two orphaned siblings with an unusually close and even slightly perverted relationship. Can you imagine playfully ripping off the towel that covers your sister's naked body and then stare at her unshaven genitals for several whole minutes? Well, Bo does that to his sister and, judging by her dubbed laughter, she doesn't mind at all. Sick, dude! Anyway, as kids they fled from Russia with their parents, but nasty soldiers brutally slaughtered mommy and daddy. A friendly smuggler took custody over them, however, and even raised and trained Bo and Ingrid into expert smugglers. When the actual plot lifts off, 20 years later, they're facing their ultimate quest as the mythical and incredibly valuable White Fire diamond is coincidentally found in a mine. Very few things in life ever made as little sense as the plot and narrative structure of \\White Fire\\\", but it sure is a lot of fun to watch. Most of the time you have no clue who's beating up who or for what cause (and I bet the actors understood even less) but whatever! The violence is magnificently grotesque and every single plot twist is pleasingly retarded. The script goes totally bonkers beyond repair when suddenly  and I won't reveal for what reason  Bo needs a replacement for Ingrid and Fred Williamson enters the scene with a big cigar in his mouth and his sleazy black fingers all over the local prostitutes. Bo's principal opponent is an Italian chick with big breasts but a hideous accent, the preposterous but catchy theme song plays at least a dozen times throughout the film, there's the obligatory \\\"we're-falling-in-love\\\" montage and loads of other attractions! My God, what a brilliant experience. The original French title translates itself as \\\"Life to Survive\\\", which is uniquely appropriate because it makes just as much sense as the rest of the movie: None!\"                                                                                                                                                                                                                            "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean strings \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    string cleaning for dataset\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# remove html tags\n",
    "for idx in range(df.review.shape[0]):\n",
    "    text = BeautifulSoup(df.review[idx])\n",
    "    texts.append(clean_str(text.get_text()))\n",
    "    labels.append(df.sentiment[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with all this stuff going down at the moment with mj ive started listening to his music, watching the odd documentary here and there, watched the wiz and watched moonwalker again. maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. some of it has subtle messages about mjs feeling towards the press and also the obvious message of drugs are bad mkay.visually impressive but of course this is all about michael jackson so unless you remotely like mj in anyway then you are going to hate this and find it boring. some may call mj an egotist for consenting to the making of this movie but mj and most of his fans would say that he made it for the fans which if true is really nice of him.the actual feature film bit when it finally starts is only on for 20 minutes or so excluding the smooth criminal sequence and joe pesci is convincing as a psychopathic all powerful drug lord. why he wants mj dead so bad is beyond me. because mj overheard his plans? nah, joe pescis character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates mjs music.lots of cool things in this like mj turning into a car and a robot and the whole speed demon sequence. also, the director must have had the patience of a saint when it came to filming the kiddy bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.bottom line, this movie is for people who like mj on one level or another (which i think is most people). if not, then stay away. it does try and give off a wholesome message and ironically mjs bestest buddy in this movie is a girl! michael jackson is truly one of the most talented people ever to grace this planet but is he guilty? well, with all the attention ive gave this subject....hmmm well i dont know because people can be different behind closed doors, i know this for a fact. he is either an extremely nice but stupid guy or one of the most sickest liars. i hope he is not the latter.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2450"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find maximum number of words in texts\n",
    "max([len(text.split()) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of number of words\n",
    "plt.hist([len(text.split()) for text in texts], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems maximum sequence length of 1000 is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239101"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find total number of unique words\n",
    "len(set(\" \".join([text for text in texts]).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81501 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 1000)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=VALIDATION_SPLIT, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1000)\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[ 9952. 10048.]\n",
      "[2548. 2452.]\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_100D = '/Users/rameda/Downloads/python/glove.6B/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embeddings\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_100D)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embeddings for the words in dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81502, 100)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            name='embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "l_cov1 = Conv1D(128, 5, activation='relu', name='conv1d_1')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5, name='pool_1')(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu', name='conv1d_2')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5, name='pool_2')(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu', name='conv1d_3')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35, name='pool_3')(l_cov3)  # global max pooling\n",
    "l_flat = Flatten(name='flatten')(l_pool3)\n",
    "l_dense = Dense(128, activation='relu', name='dense')(l_flat)\n",
    "preds = Dense(2, activation='softmax', name='softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1000, 100)         8150200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling1D)        (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling1D)        (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "pool_3 (MaxPooling1D)        (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 8,395,194\n",
      "Trainable params: 244,994\n",
      "Non-trainable params: 8,150,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 95s 5ms/step - loss: 0.6713 - acc: 0.6228 - val_loss: 1.4216 - val_acc: 0.5022\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 100s 5ms/step - loss: 0.4997 - acc: 0.7669 - val_loss: 0.4416 - val_acc: 0.7932\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 94s 5ms/step - loss: 0.4139 - acc: 0.8165 - val_loss: 0.3571 - val_acc: 0.8422\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.3668 - acc: 0.8371 - val_loss: 0.4057 - val_acc: 0.8172\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 90s 5ms/step - loss: 0.3214 - acc: 0.8604 - val_loss: 0.3474 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 90s 5ms/step - loss: 0.2736 - acc: 0.8868 - val_loss: 0.7600 - val_acc: 0.7426\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.2307 - acc: 0.9067 - val_loss: 0.9042 - val_acc: 0.6768\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 91s 5ms/step - loss: 0.1979 - acc: 0.9222 - val_loss: 0.6452 - val_acc: 0.7866\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.1606 - acc: 0.9375 - val_loss: 0.5109 - val_acc: 0.8492\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.1340 - acc: 0.9516 - val_loss: 0.5449 - val_acc: 0.8466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a29127438>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), nb_epoch=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Little complex CNN model\n",
    "As suggested in [Convolutional Neural Networks for Sentence Classification by Yoon Kim](http://www.aclweb.org/anthology/D14-1181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu',name='conv1d_1_%d'%fsz)(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5,name='pool_1_%d'%fsz)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Concatenate(axis=1, name='concat')(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu', name='conv1d_2')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5, name='pool_2')(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu', name='conv1d_3')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30, name='pool_3')(l_cov2)\n",
    "l_flat = Flatten(name='flatten')(l_pool2)\n",
    "l_dense = Dense(128, activation='relu', name='dense')(l_flat)\n",
    "preds = Dense(2, activation='softmax', name='softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1000, 100)    8150200     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1_3 (Conv1D)             (None, 998, 128)     38528       embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1_4 (Conv1D)             (None, 997, 128)     51328       embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1_5 (Conv1D)             (None, 996, 128)     64128       embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool_1_3 (MaxPooling1D)         (None, 199, 128)     0           conv1d_1_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool_1_4 (MaxPooling1D)         (None, 199, 128)     0           conv1d_1_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool_1_5 (MaxPooling1D)         (None, 199, 128)     0           conv1d_1_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 597, 128)     0           pool_1_3[0][0]                   \n",
      "                                                                 pool_1_4[0][0]                   \n",
      "                                                                 pool_1_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 593, 128)     82048       concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pool_2 (MaxPooling1D)           (None, 118, 128)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 114, 128)     82048       pool_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pool_3 (MaxPooling1D)           (None, 3, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           pool_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          49280       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 2)            258         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 8,517,818\n",
      "Trainable params: 367,618\n",
      "Non-trainable params: 8,150,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 243s 12ms/step - loss: 0.5901 - acc: 0.6635 - val_loss: 0.4070 - val_acc: 0.8182\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 243s 12ms/step - loss: 0.3743 - acc: 0.8333 - val_loss: 0.3350 - val_acc: 0.8572\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 253s 13ms/step - loss: 0.2875 - acc: 0.8791 - val_loss: 0.3377 - val_acc: 0.8546\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 261s 13ms/step - loss: 0.2180 - acc: 0.9147 - val_loss: 0.3214 - val_acc: 0.8742\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 241s 12ms/step - loss: 0.1415 - acc: 0.9482 - val_loss: 0.3406 - val_acc: 0.8592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a291542b0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), nb_epoch=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy seem to improve over simple model. But it's not a huge leap. \n",
    "\n",
    "Making embedding layers trainable would increase perfomance in my experience, but that would take a long time to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also using the sequential API for this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1000, 100)         8150200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 64)          42240     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 8,204,922\n",
      "Trainable params: 54,722\n",
      "Non-trainable params: 8,150,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "\n",
    "model1.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH, trainable=False, name='embedding'))\n",
    "model1.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4, return_sequences=True, name='lstm_1'))\n",
    "model1.add(LSTM(32, dropout=0.5, recurrent_dropout=0.5, return_sequences=False, name='lstm_2'))\n",
    "model1.add(Dense(2, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 442s 22ms/step - loss: 0.6801 - acc: 0.5625 - val_loss: 0.6265 - val_acc: 0.6604\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 435s 22ms/step - loss: 0.6167 - acc: 0.6619 - val_loss: 0.5041 - val_acc: 0.7648\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 441s 22ms/step - loss: 0.5449 - acc: 0.7343 - val_loss: 0.4215 - val_acc: 0.8156\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 419s 21ms/step - loss: 0.4923 - acc: 0.7708 - val_loss: 0.3858 - val_acc: 0.8318\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 420s 21ms/step - loss: 0.4553 - acc: 0.7937 - val_loss: 0.3608 - val_acc: 0.8474\n",
      "CPU times: user 1h 57min 49s, sys: 38min 43s, total: 2h 36min 33s\n",
      "Wall time: 35min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a291e39b0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model1.fit(X_train, y_train, validation_data=(X_val, y_val), nb_epoch=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1000, 100)         8150200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (Bidirectional)       (None, 1000, 256)         234496    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (Bidirectional)       (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 8,549,306\n",
      "Trainable params: 399,106\n",
      "Non-trainable params: 8,150,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH, trainable=False, name='embedding'))\n",
    "model2.add(Bidirectional(LSTM(128, return_sequences=True), name='lstm_1'))\n",
    "model2.add(Dropout(0.25, name='dropout_1'))\n",
    "model2.add(Bidirectional(LSTM(64, return_sequences=False), name='lstm_2'))\n",
    "model2.add(Dropout(0.25, name='dropout_2'))\n",
    "model2.add(Dense(2, activation='softmax', name='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 1108s 55ms/step - loss: 0.6188 - acc: 0.6493 - val_loss: 0.6603 - val_acc: 0.6276\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 1128s 56ms/step - loss: 0.5988 - acc: 0.6772 - val_loss: 0.5403 - val_acc: 0.7398\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 1128s 56ms/step - loss: 0.5077 - acc: 0.7671 - val_loss: 0.4983 - val_acc: 0.7496\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 1080s 54ms/step - loss: 0.4119 - acc: 0.8211 - val_loss: 0.3656 - val_acc: 0.8370\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 1103s 55ms/step - loss: 0.3525 - acc: 0.8515 - val_loss: 0.3323 - val_acc: 0.8620\n",
      "CPU times: user 8h 21min 40s, sys: 1h 43min 9s, total: 10h 4min 49s\n",
      "Wall time: 1h 32min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2a5854e0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model2.fit(X_train, y_train, validation_data=(X_val, y_val), nb_epoch=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reviews are pretty long bidirectional models are bound to perform better. More training will probably increase accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
